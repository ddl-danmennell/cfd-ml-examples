{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109844d-16b0-4cfe-8372-c14dfeefdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Stuff\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Input, \n",
    "    concatenate, \n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Conv2DTranspose,\n",
    "    ZeroPadding2D  # Move this here\n",
    ")\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten\n",
    "\n",
    "import vtk\n",
    "from vtm_data import VTK_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bbacd-8371-4ef2-bbd3-17b3d01581c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "batch_size = 32\n",
    "epochs = 5 # number of times through training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88dcc9-236d-44c2-9f0d-42c4bff33633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "class VTK_data:\n",
    "    def __init__(self, base_path, split_ratio=0.8):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.data = []\n",
    "        self.geometries = []\n",
    "        self.steady_flows = []\n",
    "        self.split_ratio = split_ratio\n",
    "        self.split_line = 0\n",
    "        \n",
    "    def load_data(self):\n",
    "        for dirpath, dirnames, filenames in os.walk(self.base_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.vtm'):\n",
    "                    full_path = Path(dirpath) / filename\n",
    "                    try:\n",
    "                        data = self._load_single_file(full_path)\n",
    "                        if data is not None:\n",
    "                            print(f\"\\nProcessing file: {filename}\")\n",
    "                            print(f\"Full path: {full_path}\")\n",
    "                            \n",
    "                            # Updated classification logic\n",
    "                            if 'geometry' in filename:\n",
    "                                print(f\"Classified as geometry file\")\n",
    "                                self.geometries.append(data)\n",
    "                            elif 'cylinder2d_iT' in filename:\n",
    "                                print(f\"Classified as flow file\")\n",
    "                                self.steady_flows.append(data)\n",
    "                            else:\n",
    "                                print(f\"Skipping file: {filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {full_path}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\nLoaded data summary:\")\n",
    "        print(f\"Number of geometry files: {len(self.geometries)}\")\n",
    "        if self.geometries:\n",
    "            print(f\"Shape of first geometry: {np.array(self.geometries[0]).shape}\")\n",
    "        \n",
    "        print(f\"Number of flow files: {len(self.steady_flows)}\")\n",
    "        if self.steady_flows:\n",
    "            print(f\"Shape of first flow: {np.array(self.steady_flows[0]).shape}\")\n",
    "        \n",
    "        total_samples = len(self.geometries)\n",
    "        self.split_line = int(total_samples * self.split_ratio)\n",
    "        \n",
    "        return self.geometries, self.steady_flows\n",
    "    \n",
    "    def _load_single_file(self, file_path):\n",
    "        reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "        reader.SetFileName(str(file_path))\n",
    "        reader.Update()\n",
    "        \n",
    "        if reader.GetErrorCode() != 0:\n",
    "            raise RuntimeError(f\"Error reading file\")\n",
    "        \n",
    "        data = reader.GetOutput()\n",
    "        if data is None:\n",
    "            raise RuntimeError(\"No data read from file\")\n",
    "            \n",
    "        data_iterator = data.NewIterator()\n",
    "        img_data = data_iterator.GetCurrentDataObject()\n",
    "        \n",
    "        if img_data is None:\n",
    "            raise RuntimeError(\"No image data found in file\")\n",
    "        \n",
    "        if hasattr(img_data, 'GetProducerPort'):\n",
    "            producer = img_data.GetProducerPort()\n",
    "            if producer:\n",
    "                producer.Update()\n",
    "        elif hasattr(img_data, 'GetSource'):\n",
    "            source = img_data.GetSource()\n",
    "            if source:\n",
    "                source.Update()\n",
    "                \n",
    "        point_data = img_data.GetPointData()\n",
    "        array_data = point_data.GetArray(0)\n",
    "        array_data = vtk.util.numpy_support.vtk_to_numpy(array_data)\n",
    "        \n",
    "        return array_data\n",
    "\n",
    "# Create instance and load data\n",
    "base_directory = \"../data\"\n",
    "dataset = VTK_data(base_directory)\n",
    "\n",
    "# Load the data\n",
    "geometries, steady_flows = dataset.load_data()\n",
    "\n",
    "if len(dataset.geometries) > 0 and len(dataset.steady_flows) > 0:\n",
    "    # Get train and test split\n",
    "    train_geometries = dataset.geometries[0:dataset.split_line]\n",
    "    train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
    "    test_geometries = dataset.geometries[dataset.split_line:-1]\n",
    "    test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
    "    \n",
    "    print(\"\\nData split sizes:\")\n",
    "    print(f\"Train geometries: {len(train_geometries)}\")\n",
    "    print(f\"Train flows: {len(train_steady_flows)}\")\n",
    "    print(f\"Test geometries: {len(test_geometries)}\")\n",
    "    print(f\"Test flows: {len(test_steady_flows)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_geometries = np.stack(train_geometries, axis=0)\n",
    "    train_steady_flows = np.stack(train_steady_flows, axis=0)\n",
    "    test_geometries = np.stack(test_geometries, axis=0)\n",
    "    test_steady_flows = np.stack(test_steady_flows, axis=0)\n",
    "    \n",
    "    print(\"\\nFinal array shapes:\")\n",
    "    print(f\"Train geometries shape: {train_geometries.shape}\")\n",
    "    print(f\"Train flows shape: {train_steady_flows.shape}\")\n",
    "    print(f\"Test geometries shape: {test_geometries.shape}\")\n",
    "    print(f\"Test flows shape: {test_steady_flows.shape}\")\n",
    "else:\n",
    "    print(\"\\nERROR: Missing either geometry or flow data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e473d2-3dbf-4329-8d70-b5e8246df54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test split\n",
    "train_geometries = dataset.geometries[0:dataset.split_line]\n",
    "train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
    "test_geometries = dataset.geometries[dataset.split_line:-1]\n",
    "test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
    "\n",
    "print(f\"Training set size: {len(train_geometries)} samples\")\n",
    "print(f\"Test set size: {len(test_geometries)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c38bf8-c5d5-4573-b30d-5f9b33918b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into single np array\n",
    "train_geometries = np.stack(train_geometries, axis=0)\n",
    "train_steady_flows = np.stack(train_steady_flows, axis=0)\n",
    "test_geometries = np.stack(test_geometries, axis=0)\n",
    "test_steady_flows = np.stack(test_steady_flows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d9d67-f4ef-45af-81c5-6b45e3e8fe7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print dataset values\n",
    "print('geometry shape:', train_geometries.shape[1:])\n",
    "print('steady flow shape:', train_steady_flows.shape[1:])\n",
    "print(train_geometries.shape[0], ' train samples')\n",
    "print(test_geometries.shape[0], ' test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd9cda-18eb-410e-8c97-8bb6b424ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten, concatenate, Conv2DTranspose, ZeroPadding2D\n",
    "\n",
    "# Define input layer to match your geometry data shape\n",
    "inputs = Input(shape=(9812,))\n",
    "\n",
    "# Initial reshape to work with convolutions\n",
    "# We reshape to (44, 223, 1) for processing\n",
    "reshaped = Reshape((44, 223, 1))(inputs)\n",
    "\n",
    "# Encoder Path (Downsampling)\n",
    "# First block\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(reshaped)\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "# Second block\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "# Third block\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "# Fourth block\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "# Bridge\n",
    "conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "# Decoder Path (Upsampling)\n",
    "# First upsampling block\n",
    "up6 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)), conv4], axis=3)\n",
    "conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "# Second upsampling block\n",
    "up7 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)), conv3], axis=3)\n",
    "conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "# Third upsampling block\n",
    "up8 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)), conv2], axis=3)\n",
    "conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "# Fourth upsampling block\n",
    "up9 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)), conv1], axis=3)\n",
    "conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "# Convolutional output before reshaping\n",
    "conv10 = Conv2D(2, (1, 1), activation='linear')(conv9)\n",
    "\n",
    "# Final reshape to match your target shape\n",
    "final_output = Reshape((9812, 2))(conv10)\n",
    "\n",
    "# Create the model with the correct output shape\n",
    "model = Model(inputs=inputs, outputs=final_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['MSE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e85f72-22a8-47c0-b545-148ef36501a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input shape (train_geometries):\", train_geometries.shape)\n",
    "print(\"Output shape (train_steady_flows):\", train_steady_flows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecd369-ac7d-4136-ae84-0cb01fc4039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2df9e-5320-46c5-abd8-fb67bd8dab7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, let's check the current shapes\n",
    "print(\"Original shapes:\")\n",
    "print(\"Train geometries shape:\", train_geometries.shape)\n",
    "print(\"Train steady flows shape:\", train_steady_flows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8430771-e656-4ed3-ba6b-5fa007f9258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the original data shapes\n",
    "model.fit(train_geometries, train_steady_flows,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_geometries, test_steady_flows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d340ff-2c59-4877-bae2-7cd146e59180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "score = model.evaluate(test_geometries, test_steady_flows, verbose=0)\n",
    "print('Average Mean Squared Error:', score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba17fa7-43a5-41c6-ab63-88ba96770582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions on test set\n",
    "predicted_steady_flow = model.predict(test_geometries, batch_size=batch_size)\n",
    "for i in xrange(predicted_steady_flow.shape[0]):\n",
    "  # plot predicted vs true flow\n",
    "  velocity_image = np.concatenate([predicted_steady_flow[i,:,:,0], test_steady_flows[i,:,:,0], test_geometries[i,:,:,0]/10.0], axis=1)\n",
    "  plt.imshow(velocity_image)\n",
    "  plt.show()\n",
    "  # plot predicted vs true pressure\n",
    "  velocity_image = np.concatenate([predicted_steady_flow[i,:,:,2], test_steady_flows[i,:,:,2], test_geometries[i,:,:,0]/10.0], axis=1)\n",
    "  plt.imshow(velocity_image)\n",
    "  plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfe9a8-4cad-4391-b6e7-b61c4c1db675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions on test set\n",
    "predicted_steady_flow = model.predict(test_geometries, batch_size=batch_size)\n",
    "\n",
    "for i in range(predicted_steady_flow.shape[0]):\n",
    "    # Note: We need to modify the concatenation since our data structure has changed\n",
    "    # Our predicted and actual data is now in shape (9812, 2) rather than (height, width, channels)\n",
    "    \n",
    "    # Reshape the data back to 2D format for visualization\n",
    "    pred_reshaped = predicted_steady_flow[i].reshape(44, 223, 2)\n",
    "    true_reshaped = test_steady_flows[i].reshape(44, 223, 2)\n",
    "    geom_reshaped = test_geometries[i].reshape(44, 223)\n",
    "    \n",
    "    # Create visualization for the flow field\n",
    "    velocity_image = np.concatenate([\n",
    "        pred_reshaped[:,:,0],      # Predicted first component\n",
    "        true_reshaped[:,:,0],      # True first component\n",
    "        geom_reshaped/10.0         # Geometry (scaled for better visualization)\n",
    "    ], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(velocity_image)\n",
    "    plt.title(f'Sample {i+1}: Predicted vs True Flow vs Geometry')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69d2ab-119d-4ed7-96ed-bd829091be28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
