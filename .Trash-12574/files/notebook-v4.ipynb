{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Predictor with MLflow Integration\n",
    "\n",
    "This notebook implements MLflow tracking for the flow predictor model with child runs for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Input, \n",
    "    concatenate, \n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Conv2DTranspose,\n",
    "    ZeroPadding2D\n",
    ")\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import vtk\n",
    "from vtm_data import VTK_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Create experiment\n",
    "mlflow.set_experiment(\"Flow_Predictor_Training\")\n",
    "\n",
    "# Enable MLflow autologging\n",
    "mlflow.keras.autolog(\n",
    "    log_models=True,\n",
    "    log_model_signatures=True,\n",
    "    log_input_examples=True,\n",
    "    registered_model_name=\"flow_predictor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "batch_size = 32\n",
    "epochs = 50 # number of times through training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "class VTK_data:\n",
    "    def __init__(self, base_path, split_ratio=0.8):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.data = []\n",
    "        self.geometries = []\n",
    "        self.steady_flows = []\n",
    "        self.split_ratio = split_ratio\n",
    "        self.split_line = 0\n",
    "        \n",
    "    def load_data(self):\n",
    "        for dirpath, dirnames, filenames in os.walk(self.base_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.vtm'):\n",
    "                    full_path = Path(dirpath) / filename\n",
    "                    try:\n",
    "                        data = self._load_single_file(full_path)\n",
    "                        if data is not None:\n",
    "                            print(f\"\\nProcessing file: {filename}\")\n",
    "                            print(f\"Full path: {full_path}\")\n",
    "                            \n",
    "                            if 'geometry' in filename:\n",
    "                                print(f\"Classified as geometry file\")\n",
    "                                self.geometries.append(data)\n",
    "                            elif 'cylinder2d_iT' in filename:\n",
    "                                print(f\"Classified as flow file\")\n",
    "                                self.steady_flows.append(data)\n",
    "                            else:\n",
    "                                print(f\"Skipping file: {filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {full_path}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\nLoaded data summary:\")\n",
    "        print(f\"Number of geometry files: {len(self.geometries)}\")\n",
    "        if self.geometries:\n",
    "            print(f\"Shape of first geometry: {np.array(self.geometries[0]).shape}\")\n",
    "        \n",
    "        print(f\"Number of flow files: {len(self.steady_flows)}\")\n",
    "        if self.steady_flows:\n",
    "            print(f\"Shape of first flow: {np.array(self.steady_flows[0]).shape}\")\n",
    "        \n",
    "        total_samples = len(self.geometries)\n",
    "        self.split_line = int(total_samples * self.split_ratio)\n",
    "        \n",
    "        return self.geometries, self.steady_flows\n",
    "    \n",
    "    def _load_single_file(self, file_path):\n",
    "        reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "        reader.SetFileName(str(file_path))\n",
    "        reader.Update()\n",
    "        \n",
    "        if reader.GetErrorCode() != 0:\n",
    "            raise RuntimeError(f\"Error reading file\")\n",
    "        \n",
    "        data = reader.GetOutput()\n",
    "        if data is None:\n",
    "            raise RuntimeError(\"No data read from file\")\n",
    "            \n",
    "        data_iterator = data.NewIterator()\n",
    "        img_data = data_iterator.GetCurrentDataObject()\n",
    "        \n",
    "        if img_data is None:\n",
    "            raise RuntimeError(\"No image data found in file\")\n",
    "        \n",
    "        if hasattr(img_data, 'GetProducerPort'):\n",
    "            producer = img_data.GetProducerPort()\n",
    "            if producer:\n",
    "                producer.Update()\n",
    "        elif hasattr(img_data, 'GetSource'):\n",
    "            source = img_data.GetSource()\n",
    "            if source:\n",
    "                source.Update()\n",
    "                \n",
    "        point_data = img_data.GetPointData()\n",
    "        array_data = point_data.GetArray(0)\n",
    "        array_data = vtk.util.numpy_support.vtk_to_numpy(array_data)\n",
    "        \n",
    "        return array_data\n",
    "\n",
    "# Create instance and load data\n",
    "base_directory = \"/mnt/data/cfd-ml-examples/sumulation\"\n",
    "dataset = VTK_data(base_directory)\n",
    "\n",
    "# Load the data\n",
    "geometries, steady_flows = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test split\n",
    "train_geometries = dataset.geometries[0:dataset.split_line]\n",
    "train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
    "test_geometries = dataset.geometries[dataset.split_line:-1]\n",
    "test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
    "\n",
    "print(f\"Training set size: {len(train_geometries)} samples\")\n",
    "print(f\"Test set size: {len(test_geometries)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into single np array\n",
    "train_geometries = np.stack(train_geometries, axis=0)\n",
    "train_steady_flows = np.stack(train_steady_flows, axis=0)\n",
    "test_geometries = np.stack(test_geometries, axis=0)\n",
    "test_steady_flows = np.stack(test_steady_flows, axis=0)\n",
    "\n",
    "# print dataset values\n",
    "print('geometry shape:', train_geometries.shape[1:])\n",
    "print('steady flow shape:', train_steady_flows.shape[1:])\n",
    "print(train_geometries.shape[0], ' train samples')\n",
    "print(test_geometries.shape[0], ' test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create model\n",
    "def create_flow_predictor_model():\n",
    "    inputs = Input(shape=(9812,))\n",
    "    reshaped = Reshape((44, 223, 1))(inputs)\n",
    "    \n",
    "    # Encoder Path\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(reshaped)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Decoder Path\n",
    "    up6 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(2, (1, 1), activation='linear')(conv9)\n",
    "    final_output = Reshape((9812, 2))(conv10)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=final_output)\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics=['MSE']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_flow_predictor_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for epoch-level MLflow tracking\n",
    "class MLflowEpochCallback(Callback):\n",
    "    def __init__(self, parent_run_id):\n",
    "        super().__init__()\n",
    "        self.parent_run_id = parent_run_id\n",
    "    \n",
    "    def on