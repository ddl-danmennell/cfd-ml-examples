{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header-cell",
            "metadata": {},
            "source": [
                "# Flow Predictor with MLflow Integration\n",
                "\n",
                "This notebook implements MLflow tracking for the flow predictor model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "import-cell",
            "metadata": {},
            "source": [
                "# Import section - add MLflow imports\n",
                "import keras\n",
                "from keras.models import Model\n",
                "from keras import backend as K\n",
                "from keras.layers import (\n",
                "    Input, \n",
                "    concatenate, \n",
                "    Conv2D,\n",
                "    MaxPooling2D,\n",
                "    Conv2DTranspose,\n",
                "    ZeroPadding2D\n",
                ")\n",
                "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten\n",
                "\n",
                "import vtk\n",
                "from vtm_data import VTK_data\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import os\n",
                "import mlflow\n",
                "import mlflow.keras\n",
                "\n",
                "# Create experiment\n",
                "mlflow.set_experiment(\"Flow_Predictor_Training\")\n",
                "\n",
                "# Enable MLflow autologging\n",
                "mlflow.keras.autolog(\n",
                "    log_models=True,\n",
                "    log_model_signatures=True,\n",
                "    log_input_examples=True,\n",
                "    registered_model_name=\"flow_predictor\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-params",
            "metadata": {},
            "source": [
                "# training params\n",
                "batch_size = 32\n",
                "epochs = 50 # number of times through training set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-loading",
            "metadata": {
                "scrolled": true
            },
            "source": [
                "# load dataset\n",
                "class VTK_data:\n",
                "    def __init__(self, base_path, split_ratio=0.8):\n",
                "        self.base_path = Path(base_path)\n",
                "        self.data = []\n",
                "        self.geometries = []\n",
                "        self.steady_flows = []\n",
                "        self.split_ratio = split_ratio\n",
                "        self.split_line = 0\n",
                "        \n",
                "    def load_data(self):\n",
                "        for dirpath, dirnames, filenames in os.walk(self.base_path):\n",
                "            for filename in filenames:\n",
                "                if filename.endswith('.vtm'):\n",
                "                    full_path = Path(dirpath) / filename\n",
                "                    try:\n",
                "                        data = self._load_single_file(full_path)\n",
                "                        if data is not None:\n",
                "                            print(f\"\\nProcessing file: {filename}\")\n",
                "                            print(f\"Full path: {full_path}\")\n",
                "                            \n",
                "                            if 'geometry' in filename:\n",
                "                                print(f\"Classified as geometry file\")\n",
                "                                self.geometries.append(data)\n",
                "                            elif 'cylinder2d_iT' in filename:\n",
                "                                print(f\"Classified as flow file\")\n",
                "                                self.steady_flows.append(data)\n",
                "                            else:\n",
                "                                print(f\"Skipping file: {filename}\")\n",
                "                    except Exception as e:\n",
                "                        print(f\"Error loading {full_path}: {str(e)}\")\n",
                "        \n",
                "        print(\"\\nLoaded data summary:\")\n",
                "        print(f\"Number of geometry files: {len(self.geometries)}\")\n",
                "        if self.geometries:\n",
                "            print(f\"Shape of first geometry: {np.array(self.geometries[0]).shape}\")\n",
                "        \n",
                "        print(f\"Number of flow files: {len(self.steady_flows)}\")\n",
                "        if self.steady_flows:\n",
                "            print(f\"Shape of first flow: {np.array(self.steady_flows[0]).shape}\")\n",
                "        \n",
                "        total_samples = len(self.geometries)\n",
                "        self.split_line = int(total_samples * self.split_ratio)\n",
                "        \n",
                "        return self.geometries, self.steady_flows\n",
                "    \n",
                "    def _load_single_file(self, file_path):\n",
                "        reader = vtk.vtkXMLMultiBlockDataReader()\n",
                "        reader.SetFileName(str(file_path))\n",
                "        reader.Update()\n",
                "        \n",
                "        if reader.GetErrorCode() != 0:\n",
                "            raise RuntimeError(f\"Error reading file\")\n",
                "        \n",
                "        data = reader.GetOutput()\n",
                "        if data is None:\n",
                "            raise RuntimeError(\"No data read from file\")\n",
                "            \n",
                "        data_iterator = data.NewIterator()\n",
                "        img_data = data_iterator.GetCurrentDataObject()\n",
                "        \n",
                "        if img_data is None:\n",
                "            raise RuntimeError(\"No image data found in file\")\n",
                "        \n",
                "        if hasattr(img_data, 'GetProducerPort'):\n",
                "            producer = img_data.GetProducerPort()\n",
                "            if producer:\n",
                "                producer.Update()\n",
                "        elif hasattr(img_data, 'GetSource'):\n",
                "            source = img_data.GetSource()\n",
                "            if source:\n",
                "                source.Update()\n",
                "                \n",
                "        point_data = img_data.GetPointData()\n",
                "        array_data = point_data.GetArray(0)\n",
                "        array_data = vtk.util.numpy_support.vtk_to_numpy(array_data)\n",
                "        \n",
                "        return array_data\n",
                "\n",
                "# Create instance and load data\n",
                "base_directory = \"/mnt/data/cfd-ml-examples/sumulation\"\n",
                "dataset = VTK_data(base_directory)\n",
                "\n",
                "# Load the data\n",
                "geometries, steady_flows = dataset.load_data()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-split",
            "metadata": {},
            "source": [
                "# get train and test split\n",
                "train_geometries = dataset.geometries[0:dataset.split_line]\n",
                "train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
                "test_geometries = dataset.geometries[dataset.split_line:-1]\n",
                "test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
                "\n",
                "print(f\"Training set size: {len(train_geometries)} samples\")\n",
                "print(f\"Test set size: {len(test_geometries)} samples\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}