{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header-cell",
            "metadata": {},
            "source": [
                "# Flow Predictor with MLflow Integration\n",
                "\n",
                "This notebook implements MLflow tracking for the flow predictor model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "import-cell",
            "metadata": {},
            "source": [
                "# Import section - add MLflow imports\n",
                "import keras\n",
                "from keras.models import Model\n",
                "from keras import backend as K\n",
                "from keras.layers import (\n",
                "    Input, \n",
                "    concatenate, \n",
                "    Conv2D,\n",
                "    MaxPooling2D,\n",
                "    Conv2DTranspose,\n",
                "    ZeroPadding2D\n",
                ")\n",
                "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten\n",
                "\n",
                "import vtk\n",
                "from vtm_data import VTK_data\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import os\n",
                "import mlflow\n",
                "import mlflow.keras\n",
                "\n",
                "# Create experiment\n",
                "mlflow.set_experiment(\"Flow_Predictor_Training\")\n",
                "\n",
                "# Enable MLflow autologging\n",
                "mlflow.keras.autolog(\n",
                "    log_models=True,\n",
                "    log_model_signatures=True,\n",
                "    log_input_examples=True,\n",
                "    registered_model_name=\"flow_predictor\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-params",
            "metadata": {},
            "source": [
                "# training params\n",
                "batch_size = 32\n",
                "epochs = 50 # number of times through training set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-loading",
            "metadata": {
                "scrolled": true
            },
            "source": [
                "# load dataset\n",
                "class VTK_data:\n",
                "    def __init__(self, base_path, split_ratio=0.8):\n",
                "        self.base_path = Path(base_path)\n",
                "        self.data = []\n",
                "        self.geometries = []\n",
                "        self.steady_flows = []\n",
                "        self.split_ratio = split_ratio\n",
                "        self.split_line = 0\n",
                "        \n",
                "    def load_data(self):\n",
                "        for dirpath, dirnames, filenames in os.walk(self.base_path):\n",
                "            for filename in filenames:\n",
                "                if filename.endswith('.vtm'):\n",
                "                    full_path = Path(dirpath) / filename\n",
                "                    try:\n",
                "                        data = self._load_single_file(full_path)\n",
                "                        if data is not None:\n",
                "                            print(f\"\\nProcessing file: {filename}\")\n",
                "                            print(f\"Full path: {full_path}\")\n",
                "                            \n",
                "                            if 'geometry' in filename:\n",
                "                                print(f\"Classified as geometry file\")\n",
                "                                self.geometries.append(data)\n",
                "                            elif 'cylinder2d_iT' in filename:\n",
                "                                print(f\"Classified as flow file\")\n",
                "                                self.steady_flows.append(data)\n",
                "                            else:\n",
                "                                print(f\"Skipping file: {filename}\")\n",
                "                    except Exception as e:\n",
                "                        print(f\"Error loading {full_path}: {str(e)}\")\n",
                "        \n",
                "        print(\"\\nLoaded data summary:\")\n",
                "        print(f\"Number of geometry files: {len(self.geometries)}\")\n",
                "        if self.geometries:\n",
                "            print(f\"Shape of first geometry: {np.array(self.geometries[0]).shape}\")\n",
                "        \n",
                "        print(f\"Number of flow files: {len(self.steady_flows)}\")\n",
                "        if self.steady_flows:\n",
                "            print(f\"Shape of first flow: {np.array(self.steady_flows[0]).shape}\")\n",
                "        \n",
                "        total_samples = len(self.geometries)\n",
                "        self.split_line = int(total_samples * self.split_ratio)\n",
                "        \n",
                "        return self.geometries, self.steady_flows\n",
                "    \n",
                "    def _load_single_file(self, file_path):\n",
                "        reader = vtk.vtkXMLMultiBlockDataReader()\n",
                "        reader.SetFileName(str(file_path))\n",
                "        reader.Update()\n",
                "        \n",
                "        if reader.GetErrorCode() != 0:\n",
                "            raise RuntimeError(f\"Error reading file\")\n",
                "        \n",
                "        data = reader.GetOutput()\n",
                "        if data is None:\n",
                "            raise RuntimeError(\"No data read from file\")\n",
                "            \n",
                "        data_iterator = data.NewIterator()\n",
                "        img_data = data_iterator.GetCurrentDataObject()\n",
                "        \n",
                "        if img_data is None:\n",
                "            raise RuntimeError(\"No image data found in file\")\n",
                "        \n",
                "        if hasattr(img_data, 'GetProducerPort'):\n",
                "            producer = img_data.GetProducerPort()\n",
                "            if producer:\n",
                "                producer.Update()\n",
                "        elif hasattr(img_data, 'GetSource'):\n",
                "            source = img_data.GetSource()\n",
                "            if source:\n",
                "                source.Update()\n",
                "                \n",
                "        point_data = img_data.GetPointData()\n",
                "        array_data = point_data.GetArray(0)\n",
                "        array_data = vtk.util.numpy_support.vtk_to_numpy(array_data)\n",
                "        \n",
                "        return array_data\n",
                "\n",
                "# Create instance and load data\n",
                "base_directory = \"/mnt/data/cfd-ml-examples/sumulation\"\n",
                "dataset = VTK_data(base_directory)\n",
                "\n",
                "# Load the data\n",
                "geometries, steady_flows = dataset.load_data()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-split",
            "metadata": {},
            "source": [
                "# get train and test split\n",
                "train_geometries = dataset.geometries[0:dataset.split_line]\n",
                "train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
                "test_geometries = dataset.geometries[dataset.split_line:-1]\n",
                "test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
                "\n",
                "print(f\"Training set size: {len(train_geometries)} samples\")\n",
                "print(f\"Test set size: {len(test_geometries)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-reshape",
            "metadata": {},
            "source": [
                "# reshape into single np array\n",
                "train_geometries = np.stack(train_geometries, axis=0)\n",
                "train_steady_flows = np.stack(train_steady_flows, axis=0)\n",
                "test_geometries = np.stack(test_geometries, axis=0)\n",
                "test_steady_flows = np.stack(test_steady_flows, axis=0)\n",
                "\n",
                "# print dataset values\n",
                "print('geometry shape:', train_geometries.shape[1:])\n",
                "print('steady flow shape:', train_steady_flows.shape[1:])\n",
                "print(train_geometries.shape[0], ' train samples')\n",
                "print(test_geometries.shape[0], ' test samples')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model-definition",
            "metadata": {},
            "source": [
                "# Define and create model\n",
                "def create_flow_predictor_model():\n",
                "    inputs = Input(shape=(9812,))\n",
                "    reshaped = Reshape((44, 223, 1))(inputs)\n",
                "    \n",
                "    # Encoder Path\n",
                "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(reshaped)\n",
                "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
                "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
                "    \n",
                "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
                "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
                "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
                "    \n",
                "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
                "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
                "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
                "    \n",
                "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
                "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
                "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
                "    \n",
                "    # Bridge\n",
                "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
                "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
                "    \n",
                "    # Decoder Path\n",
                "    up6 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)), conv4], axis=3)\n",
                "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
                "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
                "    \n",
                "    up7 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)), conv3], axis=3)\n",
                "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
                "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
                "    \n",
                "    up8 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)), conv2], axis=3)\n",
                "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
                "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
                "    \n",
                "    up9 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)), conv1], axis=3)\n",
                "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
                "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
                "    \n",
                "    conv10 = Conv2D(2, (1, 1), activation='linear')(conv9)\n",
                "    final_output = Reshape((9812, 2))(conv10)\n",
                "    \n",
                "    model = Model(inputs=inputs, outputs=final_output)\n",
                "    model.compile(\n",
                "        loss='mse',\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
                "        metrics=['MSE']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "# Create model\n",
                "model = create_flow_predictor_model()\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mlflow-training",
            "metadata": {},
            "source": [
                "        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mlflow-callback",
            "metadata": {},
            "source": [
                "# Create a custom Keras callback for epoch-level MLflow tracking\n",
                "class MLflowEpochCallback(keras.callbacks.Callback):\n",
                "    def __init__(self, parent_run_id):\n",
                "        super().__init__()\n",
                "        self.parent_run_id = parent_run_id\n",
                "        \n",
                "    def on_epoch_begin(self, epoch, logs=None):\n",
                "        mlflow.start_run(run_name=f'epoch_{epoch}', nested=True)\n",
                "        mlflow.log_param('epoch', epoch)\n",
                "        \n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        if logs is None:\n",
                "            logs = {}\n",
                "            \n",
                "        # Log metrics for this epoch\n",
                "        for metric_name, metric_value in logs.items():\n",
                "            mlflow.log_metric(metric_name, metric_value)\n",
                "        \n",
                "        # Generate and log sample predictions for this epoch\n",
                "        predicted_steady_flow = self.model.predict(test_geometries[:1], batch_size=batch_size)\n",
                "        \n",
                "        pred_reshaped = predicted_steady_flow[0].reshape(44, 223, 2)\n",
                "        true_reshaped = test_steady_flows[0].reshape(44, 223, 2)\n",
                "        geom_reshaped = test_geometries[0].reshape(44, 223)\n",
                "        \n",
                "        plt.figure(figsize=(15, 5))\n",
                "        velocity_image = np.concatenate([\n",
                "            pred_reshaped[:,:,0],\n",
                "            true_reshaped[:,:,0],\n",
                "            geom_reshaped/10.0\n",
                "        ], axis=1)\n",
                "        \n",
                "        plt.imshow(velocity_image)\n",
                "        plt.title(f'Epoch {epoch}: Predicted vs True Flow vs Geometry')\n",
                "        plt.colorbar()\n",
                "        \n",
                "        # Save and log figure for this epoch\n",
                "        plt.savefig(f'prediction_epoch_{epoch}.png')\n",
                "        mlflow.log_artifact(f'prediction_epoch_{epoch}.png')\n",
                "        plt.close()\n",
                "        \n",
                "        mlflow.end_run()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mlflow-training",
            "metadata": {},
            "source": [
                "print('Starting MLflow run for model training...')\n",
                "\n",
                "# Train model with MLflow tracking\n",
                "with mlflow.start_run(run_name='flow_predictor_training') as parent_run:\n",
                "    # Log parameters\n",
                "    mlflow.log_param('batch_size', batch_size)\n",
                "    mlflow.log_param('epochs', epochs)\n",
                "    mlflow.log_param('learning_rate', 1e-4)\n",
                "    mlflow.log_param('train_samples', len(train_geometries))\n",
                "    mlflow.log_param('test_samples', len(test_geometries))\n",
                "    \n",
                "    # Create callback for epoch-level tracking\n",
                "    mlflow_callback = MLflowEpochCallback(parent_run.info.run_id)\n",
                "    \n",
                "    # Train model with callback\n",
                "    history = model.fit(\n",
                "        train_geometries, \n",
                "        train_steady_flows,\n",
                "        batch_size=batch_size,\n",
                "        epochs=epochs,\n",
                "        verbose=1,\n",
                "        validation_data=(test_geometries, test_steady_flows),\n",
                "        callbacks=[mlflow_callback]\n",
                "    )\n",
                "    \n",
                "    # Evaluate on test set\n",
                "    test_loss = model.evaluate(test_geometries, test_steady_flows, verbose=0)\n",
                "    mlflow.log_metric('test_mse', test_loss[0])\n",
                "    print('\\nAverage Mean Squared Error:', test_loss[0])\n",
                "    \n",
                "    # Register the model if it's the best one so far\n",
                "    print('\\nChecking if current model is best performing...')\n",
                "    client = mlflow.tracking.MlflowClient()\n",
                "    runs = client.search_runs(\n",
                "        experiment_ids=[parent_run.info.experiment_id],\n",
                "        order_by=['metrics.test_mse ASC']\n",
                "    )\n",
                "    \n",
                "    if parent_run.info.run_id == runs[0].info.run_id:\n",
                "        print('New best model found! Registering model...')\n",
                "        mlflow.keras.log_model(\n",
                "            model,\n",
                "            'model',\n",
                "            registered_model_name='flow_predictor',\n",
                "            signature=mlflow.models.infer_signature(\n",
                "                train_geometries[:2],\n",
                "                model.predict(train_geometries[:2])\n",
                "            )\n",
                "        )\n",
                "        print('Model registered successfully!')\n",
                "    else:\n",
                "        print('Not best model - skipping registration')\n",
                "\n",
                "print('\\nTraining and logging complete! Check MLflow UI for detailed metrics and artifacts.')"
            ]
        }\n",
                "    \n",
                "    # Evaluate on test set\n",
                "    test_loss = model.evaluate(test_geometries, test_steady_flows, verbose=0)\n",
                "    mlflow.log_metric(\"test_mse\", test_loss[0])\n",
                "    print('\\nAverage Mean Squared Error:', test_loss[0])\n",
                "    \n",
                "    # Generate and log sample predictions visualization\n",
                "    print(\"\\nGenerating prediction visualizations...\")\n",
                "    predicted_steady_flow = model.predict(test_geometries[:3], batch_size=batch_size)\n",
                "    \n",
                "    for i in range(3):\n",
                "        pred_reshaped = predicted_steady_flow[i].reshape(44, 223, 2)\n",
                "        true_reshaped = test_steady_flows[i].reshape(44, 223, 2)\n",
                "        geom_reshaped = test_geometries[i].reshape(44, 223)\n",
                "        \n",
                "        plt.figure(figsize=(15, 5))\n",
                "        velocity_image = np.concatenate([\n",
                "            pred_reshaped[:,:,0],\n",
                "            true_reshaped[:,:,0],\n",
                "            geom_reshaped/10.0\n",
                "        ], axis=1)\n",
                "        \n",
                "        plt.imshow(velocity_image)\n",
                "        plt.title(f'Sample {i+1}: Predicted vs True Flow vs Geometry')\n",
                "        plt.colorbar()\n",
                "        \n",
                "        # Save and log figure\n",
                "        plt.savefig(f'prediction_sample_{i}.png')\n",
                "        mlflow.log_artifact(f'prediction_sample_{i}.png')\n",
                "        plt.close()\n",
                "    \n",
                "    # Register the model if it's the best one so far\n",
                "    print(\"\\nChecking if current model is best performing...\")\n",
                "    client = mlflow.tracking.MlflowClient()\n",
                "    runs = client.search_runs(\n",
                "        experiment_ids=[run.info.experiment_id],\n",
                "        order_by=[\"metrics.test_mse ASC\"]\n",
                "    )\n",
                "    \n",
                "    if run.info.run_id == runs[0].info.run_id:\n",
                "        print(\"New best model found! Registering model...\")\n",
                "        mlflow.keras.log_model(\n",
                "            model,\n",
                "            \"model\",\n",
                "            registered_model_name=\"flow_predictor\",\n",
                "            signature=mlflow.models.infer_signature(\n",
                "                train_geometries[:2],\n",
                "                model.predict(train_geometries[:2])\n",
                "            )\n",
                "        )\n",
                "        print(\"Model registered successfully!\")\n",
                "    else:\n",
                "        print(\"Not best model - skipping registration\")\n",
                "\n",
                "print('\\nTraining and logging complete! Check MLflow UI for detailed metrics and artifacts.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}