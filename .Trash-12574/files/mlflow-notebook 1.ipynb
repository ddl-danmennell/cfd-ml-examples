{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109844d-16b0-4cfe-8372-c14dfeefdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section - add MLflow imports\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Input, \n",
    "    concatenate, \n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Conv2DTranspose,\n",
    "    ZeroPadding2D\n",
    ")\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Flatten\n",
    "\n",
    "import vtk\n",
    "from vtm_data import VTK_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Set MLflow tracking URI - Domino configures this automatically\n",
    "# mlflow.set_tracking_uri(\"http://localhost:80\")  \n",
    "\n",
    "# Create experiment\n",
    "mlflow.set_experiment(\"Flow_Predictor_Training\")\n",
    "\n",
    "# Enable MLflow autologging\n",
    "mlflow.keras.autolog(\n",
    "    log_models=True,\n",
    "    log_model_signatures=True,\n",
    "    log_input_examples=True,\n",
    "    registered_model_name=\"flow_predictor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bbacd-8371-4ef2-bbd3-17b3d01581c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "batch_size = 32\n",
    "epochs = 50 # number of times through training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88dcc9-236d-44c2-9f0d-42c4bff33633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "class VTK_data:\n",
    "    def __init__(self, base_path, split_ratio=0.8):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.data = []\n",
    "        self.geometries = []\n",
    "        self.steady_flows = []\n",
    "        self.split_ratio = split_ratio\n",
    "        self.split_line = 0\n",
    "        \n",
    "    def load_data(self):\n",
    "        for dirpath, dirnames, filenames in os.walk(self.base_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.vtm'):\n",
    "                    full_path = Path(dirpath) / filename\n",
    "                    try:\n",
    "                        data = self._load_single_file(full_path)\n",
    "                        if data is not None:\n",
    "                            print(f\"\\nProcessing file: {filename}\")\n",
    "                            print(f\"Full path: {full_path}\")\n",
    "                            \n",
    "                            # Updated classification logic\n",
    "                            if 'geometry' in filename:\n",
    "                                print(f\"Classified as geometry file\")\n",
    "                                self.geometries.append(data)\n",
    "                            elif 'cylinder2d_iT' in filename:\n",
    "                                print(f\"Classified as flow file\")\n",
    "                                self.steady_flows.append(data)\n",
    "                            else:\n",
    "                                print(f\"Skipping file: {filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {full_path}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\nLoaded data summary:\")\n",
    "        print(f\"Number of geometry files: {len(self.geometries)}\")\n",
    "        if self.geometries:\n",
    "            print(f\"Shape of first geometry: {np.array(self.geometries[0]).shape}\")\n",
    "        \n",
    "        print(f\"Number of flow files: {len(self.steady_flows)}\")\n",
    "        if self.steady_flows:\n",
    "            print(f\"Shape of first flow: {np.array(self.steady_flows[0]).shape}\")\n",
    "        \n",
    "        total_samples = len(self.geometries)\n",
    "        self.split_line = int(total_samples * self.split_ratio)\n",
    "        \n",
    "        return self.geometries, self.steady_flows\n",
    "    \n",
    "    def _load_single_file(self, file_path):\n",
    "        reader = vtk.vtkXMLMultiBlockDataReader()\n",
    "        reader.SetFileName(str(file_path))\n",
    "        reader.Update()\n",
    "        \n",
    "        if reader.GetErrorCode() != 0:\n",
    "            raise RuntimeError(f\"Error reading file\")\n",
    "        \n",
    "        data = reader.GetOutput()\n",
    "        if data is None:\n",
    "            raise RuntimeError(\"No data read from file\")\n",
    "            \n",
    "        data_iterator = data.NewIterator()\n",
    "        img_data = data_iterator.GetCurrentDataObject()\n",
    "        \n",
    "        if img_data is None:\n",
    "            raise RuntimeError(\"No image data found in file\")\n",
    "        \n",
    "        if hasattr(img_data, 'GetProducerPort'):\n",
    "            producer = img_data.GetProducerPort()\n",
    "            if producer:\n",
    "                producer.Update()\n",
    "        elif hasattr(img_data, 'GetSource'):\n",
    "            source = img_data.GetSource()\n",
    "            if source:\n",
    "                source.Update()\n",
    "                \n",
    "        point_data = img_data.GetPointData()\n",
    "        array_data = point_data.GetArray(0)\n",
    "        array_data = vtk.util.numpy_support.vtk_to_numpy(array_data)\n",
    "        \n",
    "        return array_data\n",
    "\n",
    "# Create instance and load data\n",
    "base_directory = \"/mnt/data/cfd-ml-examples/sumulation\"\n",
    "dataset = VTK_data(base_directory)\n",
    "\n",
    "# Load the data\n",
    "geometries, steady_flows = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e473d2-3dbf-4329-8d70-b5e8246df54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test split\n",
    "train_geometries = dataset.geometries[0:dataset.split_line]\n",
    "train_steady_flows = dataset.steady_flows[0:dataset.split_line]\n",
    "test_geometries = dataset.geometries[dataset.split_line:-1]\n",
    "test_steady_flows = dataset.steady_flows[dataset.split_line:-1]\n",
    "\n",
    "print(f\"Training set size: {len(train_geometries)} samples\")\n",
    "print(f\"Test set size: {len(test_geometries)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c38bf8-c5d5-4573-b30d-5fa9b33918b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into single np array\n",
    "train_geometries = np.stack(train_geometries, axis=0)\n",
    "train_steady_flows = np.stack(train_steady_flows, axis=0)\n",
    "test_geometries = np.stack(test_geometries, axis=0)\n",
    "test_steady_flows = np.stack(test_steady_flows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d9d67-f4ef-45af-81c5-6b45e3e8fe7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print dataset values\n",
    "print('geometry shape:', train_geometries.shape[1:])\n",
    "print('steady flow shape:', train_steady_flows.shape[1:])\n",
    "print(train_geometries.shape[0], ' train samples')\n",
    "print(test_geometries.shape[0], ' test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd9cda-18eb-410e-8c97-8bb6b424ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and create model\n",
    "def create_flow_predictor_model():\n",
    "    inputs = Input(shape=(9812,))\n",
    "    reshaped = Reshape((44, 223, 1))(inputs)\n",
    "    \n",
    "    # Encoder Path\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(reshaped)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Decoder Path\n",
    "    up6 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = concatenate([ZeroPadding2D(((1,0),(1,0)))(Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = concatenate([ZeroPadding2D(((0,0),(1,0)))(Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8)), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(2, (1, 1), activation='linear')(conv9)\n",
    "    final_output = Reshape((9812, 2))(conv10)
    
    model = Model(inputs=inputs, outputs=final_output)
    model.compile(
        loss='mse',
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        metrics=['MSE']
    )
    return model

# Create model
model = create_flow_predictor_model()
model.summary()

print("\\nStarting MLflow run for model training...")

# Train model with MLflow tracking
with mlflow.start_run(run_name="flow_predictor_training") as run:
    # Log parameters
    mlflow.log_param("batch_size", batch_size)
    mlflow.log_param("epochs", epochs)
    mlflow.log_param("learning_rate", 1e-4)
    mlflow.log_param("train_samples", len(train_geometries))
    mlflow.log_param("test_samples", len(test_geometries))
    
    # Train model - MLflow autolog will capture metrics
    history = model.fit(
        train_geometries, 
        train_steady_flows,
        batch_size=batch_size,
        epochs=epochs,
        verbose=1,
        validation_data=(test_geometries, test_steady_flows)
    )
    
    # Evaluate on test set
    test_loss = model.evaluate(test_geometries, test_steady_flows, verbose=0)
    mlflow.log_metric("test_mse", test_loss[0])
    print('\\nAverage Mean Squared Error:', test_loss[0])
    
    # Generate and log sample predictions visualization
    print("\\nGenerating prediction visualizations...")
    predicted_steady_flow = model.predict(test_geometries[:3], batch_size=batch_size)
    
    for i in range(3):
        pred_reshaped = predicted_steady_flow[i].reshape(44, 223, 2)
        true_reshaped = test_steady_flows[i].reshape(44, 223, 2)
        geom_reshaped = test_geometries[i].reshape(44, 223)
        
        plt.figure(figsize=(15, 5))
        velocity_image = np.concatenate([
            pred_reshaped[:,:,0],
            true_reshaped[:,:,0],
            geom_reshaped/10.0
        ], axis=1)
        
        plt.imshow(velocity_image)
        plt.title(f'Sample {i+1}: Predicted vs True Flow vs Geometry')
        plt.colorbar()
        
        # Save and log figure
        plt.savefig(f'prediction_sample_{i}.png')
        mlflow.log_artifact(f'prediction_sample_{i}.png')
        plt.close()
    
    # Register the model if it's the best one so far
    print("\\nChecking if current model is best performing...")
    client = mlflow.tracking.MlflowClient()
    runs = client.search_runs(
        experiment_ids=[run.info.experiment_id],
        order_by=["metrics.test_mse ASC"]
    )
    
    if run.info.run_id == runs[0].info.run_id:
        print("New best model found! Registering model...")
        mlflow.keras.log_model(
            model,
            "model",
            registered_model_name="flow_predictor",
            signature=mlflow.models.infer_signature(
                train_geometries[:2],
                model.predict(train_geometries[:2])
            )
        )
        print("Model registered successfully!")
    else:
        print("Not best model - skipping registration")

print('\\nTraining and logging complete! Check MLflow UI for detailed metrics and artifacts.')